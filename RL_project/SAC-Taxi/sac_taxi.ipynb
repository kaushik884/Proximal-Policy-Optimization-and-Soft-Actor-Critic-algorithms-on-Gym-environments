{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqOIMUNRRriK",
        "outputId": "aefeb206-88f3-4695-d2d2-b2d151e6dde0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.9 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.4/1.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.3.0\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (4.3.0)\n",
            "Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376417 sha256=424507693dc6d19c57c639ff5d88bbb552069960e4fa4e9e7101323168926bf2\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.6.1\n",
            "    Uninstalling pygame-2.6.1:\n",
            "      Successfully uninstalled pygame-2.6.1\n",
            "Successfully installed box2d-py-2.3.5 pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEU4wnLrRgrX"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "gamma = 0.9  # Discount factor\n",
        "tau = 0.005  # Target smoothing coefficient\n",
        "alpha = 0.9  # Initial temperature for entropy\n",
        "lr = 5e-4    # Learning rate\n",
        "buffer_size = int(1e5)  # Replay buffer size\n",
        "batch_size = 64  # Mini-batch size"
      ],
      "metadata": {
        "id": "XGjPo9JYRwfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SAC Networks\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc3(x)  # No activation for Q-values\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        logits = self.fc3(x)\n",
        "        return logits\n",
        "\n",
        "# Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "aw5NnA4bSL4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# SAC Training\n",
        "def train_sac(env, num_episodes=1000, num_frames=6):\n",
        "    state_dim = env.observation_space.n * num_frames\n",
        "    action_dim = env.action_space.n\n",
        "\n",
        "    # Networks\n",
        "    q1 = QNetwork(state_dim, action_dim)\n",
        "    q2 = QNetwork(state_dim, action_dim)\n",
        "    target_q1 = QNetwork(state_dim, action_dim)\n",
        "    target_q2 = QNetwork(state_dim, action_dim)\n",
        "    policy = PolicyNetwork(state_dim, action_dim)\n",
        "\n",
        "    target_q1.load_state_dict(q1.state_dict())\n",
        "    target_q2.load_state_dict(q2.state_dict())\n",
        "\n",
        "    # Optimizers\n",
        "    q1_optimizer = optim.Adam(q1.parameters(), lr=lr)\n",
        "    q2_optimizer = optim.Adam(q2.parameters(), lr=lr)\n",
        "    policy_optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
        "    alpha_optimizer = optim.Adam([torch.tensor(alpha, requires_grad=True)], lr=lr)\n",
        "\n",
        "    # Replay buffer\n",
        "    replay_buffer = ReplayBuffer(buffer_size)\n",
        "    episode_rewards = []\n",
        "    for episode in range(num_episodes):\n",
        "        frames = deque(maxlen=num_frames)  # Store the last `num_frames`\n",
        "        state = env.reset()\n",
        "        for _ in range(num_frames):  # Fill initial frames with the same state\n",
        "            frames.append(state)\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        while not done:\n",
        "            # Stack frames along the feature dimension\n",
        "            stacked_frames = torch.cat([torch.eye(env.observation_space.n, dtype=torch.float32)[s] for s in frames], dim=0)\n",
        "            state_tensor = stacked_frames.unsqueeze(0)\n",
        "            logits = policy(state_tensor)\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            action = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            # Take action in the environment\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            frames.append(next_state)  # Update frame buffer\n",
        "\n",
        "            # Add experience to replay buffer\n",
        "            replay_buffer.add((list(frames), action, reward, list(frames), done))\n",
        "            episode_reward += reward\n",
        "            episode_rewards.append(episode_reward)\n",
        "\n",
        "            if len(replay_buffer) >= batch_size:\n",
        "                # Sample from replay buffer\n",
        "                batch = replay_buffer.sample(batch_size)\n",
        "                states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "                # Process sampled batch\n",
        "                states = torch.stack([\n",
        "                    torch.cat([torch.eye(env.observation_space.n, dtype=torch.float32)[s] for s in state_seq], dim=0)\n",
        "                    for state_seq in states\n",
        "                ])\n",
        "                next_states = torch.stack([\n",
        "                    torch.cat([torch.eye(env.observation_space.n, dtype=torch.float32)[s] for s in next_state_seq], dim=0)\n",
        "                    for next_state_seq in next_states\n",
        "                ])\n",
        "                actions = torch.tensor(actions, dtype=torch.long)\n",
        "                rewards = torch.tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
        "                dones = torch.tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
        "                # next_states = torch.tensor(next_states, dtype=torch.float32)\n",
        "                # dones = torch.tensor(dones, dtype=torch.float32)\n",
        "\n",
        "                # Compute target Q values\n",
        "                with torch.no_grad():\n",
        "                    next_logits = policy(next_states)\n",
        "                    next_probs = torch.softmax(next_logits, dim=-1)\n",
        "                    next_entropy = -torch.sum(next_probs * torch.log(next_probs + 1e-10), dim=-1)\n",
        "\n",
        "                    target_q1_values = target_q1(next_states)\n",
        "                    target_q2_values = target_q2(next_states)\n",
        "                    target_q_values = torch.min(target_q1_values, target_q2_values)\n",
        "                    target_values = rewards + gamma * (1 - dones) * (target_q_values + alpha * next_entropy.unsqueeze(1))\n",
        "\n",
        "                target_values = target_values.gather(1, actions.unsqueeze(-1)).squeeze(-1)  # Match shape to Q-values\n",
        "\n",
        "                # Update Q Networks\n",
        "                q1_values = q1(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "                q2_values = q2(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "                q1_loss = torch.mean((q1_values - target_values) ** 2)\n",
        "                q2_loss = torch.mean((q2_values - target_values) ** 2)\n",
        "\n",
        "                q1_optimizer.zero_grad()\n",
        "                q1_loss.backward()\n",
        "                q1_optimizer.step()\n",
        "\n",
        "                q2_optimizer.zero_grad()\n",
        "                q2_loss.backward()\n",
        "                q2_optimizer.step()\n",
        "\n",
        "                # Update Policy Network\n",
        "                logits = policy(states)\n",
        "                probs = torch.softmax(logits, dim=-1)\n",
        "                entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1).unsqueeze(1)\n",
        "\n",
        "                q_values = torch.min(q1(states), q2(states))\n",
        "                policy_loss = torch.mean(-probs * (q_values + alpha * entropy))\n",
        "\n",
        "                policy_optimizer.zero_grad()\n",
        "                policy_loss.backward()\n",
        "                policy_optimizer.step()\n",
        "\n",
        "                # Update target Q Networks\n",
        "                for target_param, param in zip(target_q1.parameters(), q1.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "                for target_param, param in zip(target_q2.parameters(), q2.parameters()):\n",
        "                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "        print(f\"Episode {episode + 1}, Reward: {episode_reward}\")\n",
        "    return episode_rewards"
      ],
      "metadata": {
        "id": "Vme-Bk3gSRB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main\n",
        "env = gym.make('Taxi-v3')\n",
        "episode_rewards = train_sac(env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A-QDtkDiSVrO",
        "outputId": "f1f3889c-4bdf-40f1-a57c-75efc71301e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Reward: -533\n",
            "Episode 2, Reward: -218\n",
            "Episode 3, Reward: -209\n",
            "Episode 4, Reward: -236\n",
            "Episode 5, Reward: -218\n",
            "Episode 6, Reward: -209\n",
            "Episode 7, Reward: -200\n",
            "Episode 8, Reward: -200\n",
            "Episode 9, Reward: -200\n",
            "Episode 10, Reward: -200\n",
            "Episode 11, Reward: -200\n",
            "Episode 12, Reward: -200\n",
            "Episode 13, Reward: -200\n",
            "Episode 14, Reward: -200\n",
            "Episode 15, Reward: -200\n",
            "Episode 16, Reward: -200\n",
            "Episode 17, Reward: -200\n",
            "Episode 18, Reward: -200\n",
            "Episode 19, Reward: -200\n",
            "Episode 20, Reward: -200\n",
            "Episode 21, Reward: -200\n",
            "Episode 22, Reward: -200\n",
            "Episode 23, Reward: -200\n",
            "Episode 24, Reward: -200\n",
            "Episode 25, Reward: -200\n",
            "Episode 26, Reward: -200\n",
            "Episode 27, Reward: -200\n",
            "Episode 28, Reward: -200\n",
            "Episode 29, Reward: -200\n",
            "Episode 30, Reward: -200\n",
            "Episode 31, Reward: -200\n",
            "Episode 32, Reward: -200\n",
            "Episode 33, Reward: -200\n",
            "Episode 34, Reward: -200\n",
            "Episode 35, Reward: -200\n",
            "Episode 36, Reward: -200\n",
            "Episode 37, Reward: -200\n",
            "Episode 38, Reward: -200\n",
            "Episode 39, Reward: -200\n",
            "Episode 40, Reward: -200\n",
            "Episode 41, Reward: -200\n",
            "Episode 42, Reward: -200\n",
            "Episode 43, Reward: -200\n",
            "Episode 44, Reward: -200\n",
            "Episode 45, Reward: -200\n",
            "Episode 46, Reward: -200\n",
            "Episode 47, Reward: -200\n",
            "Episode 48, Reward: -200\n",
            "Episode 49, Reward: -200\n",
            "Episode 50, Reward: -200\n",
            "Episode 51, Reward: -200\n",
            "Episode 52, Reward: -200\n",
            "Episode 53, Reward: -200\n",
            "Episode 54, Reward: -200\n",
            "Episode 55, Reward: -200\n",
            "Episode 56, Reward: -200\n",
            "Episode 57, Reward: -200\n",
            "Episode 58, Reward: -200\n",
            "Episode 59, Reward: -200\n",
            "Episode 60, Reward: -200\n",
            "Episode 61, Reward: -200\n",
            "Episode 62, Reward: -200\n",
            "Episode 63, Reward: -200\n",
            "Episode 64, Reward: -200\n",
            "Episode 65, Reward: -200\n",
            "Episode 66, Reward: -200\n",
            "Episode 67, Reward: -200\n",
            "Episode 68, Reward: -200\n",
            "Episode 69, Reward: -200\n",
            "Episode 70, Reward: -200\n",
            "Episode 71, Reward: -200\n",
            "Episode 72, Reward: -200\n",
            "Episode 73, Reward: -200\n",
            "Episode 74, Reward: -200\n",
            "Episode 75, Reward: -200\n",
            "Episode 76, Reward: -200\n",
            "Episode 77, Reward: -200\n",
            "Episode 78, Reward: -200\n",
            "Episode 79, Reward: -200\n",
            "Episode 80, Reward: -200\n",
            "Episode 81, Reward: -200\n",
            "Episode 82, Reward: -200\n",
            "Episode 83, Reward: -200\n",
            "Episode 84, Reward: -200\n",
            "Episode 85, Reward: -200\n",
            "Episode 86, Reward: -200\n",
            "Episode 87, Reward: -200\n",
            "Episode 88, Reward: -200\n",
            "Episode 89, Reward: -200\n",
            "Episode 90, Reward: -200\n",
            "Episode 91, Reward: -200\n",
            "Episode 92, Reward: -200\n",
            "Episode 93, Reward: -200\n",
            "Episode 94, Reward: -200\n",
            "Episode 95, Reward: -200\n",
            "Episode 96, Reward: -200\n",
            "Episode 97, Reward: -200\n",
            "Episode 98, Reward: -200\n",
            "Episode 99, Reward: -200\n",
            "Episode 100, Reward: -200\n",
            "Episode 101, Reward: -200\n",
            "Episode 102, Reward: -200\n",
            "Episode 103, Reward: -200\n",
            "Episode 104, Reward: -200\n",
            "Episode 105, Reward: -200\n",
            "Episode 106, Reward: -200\n",
            "Episode 107, Reward: -200\n",
            "Episode 108, Reward: -200\n",
            "Episode 109, Reward: -200\n",
            "Episode 110, Reward: -200\n",
            "Episode 111, Reward: -200\n",
            "Episode 112, Reward: -200\n",
            "Episode 113, Reward: -200\n",
            "Episode 114, Reward: -200\n",
            "Episode 115, Reward: -200\n",
            "Episode 116, Reward: -200\n",
            "Episode 117, Reward: -200\n",
            "Episode 118, Reward: -200\n",
            "Episode 119, Reward: -200\n",
            "Episode 120, Reward: -200\n",
            "Episode 121, Reward: -200\n",
            "Episode 122, Reward: -200\n",
            "Episode 123, Reward: -200\n",
            "Episode 124, Reward: -200\n",
            "Episode 125, Reward: -200\n",
            "Episode 126, Reward: -200\n",
            "Episode 127, Reward: -200\n",
            "Episode 128, Reward: -200\n",
            "Episode 129, Reward: -200\n",
            "Episode 130, Reward: -200\n",
            "Episode 131, Reward: -200\n",
            "Episode 132, Reward: -200\n",
            "Episode 133, Reward: -200\n",
            "Episode 134, Reward: -200\n",
            "Episode 135, Reward: -200\n",
            "Episode 136, Reward: -200\n",
            "Episode 137, Reward: -200\n",
            "Episode 138, Reward: -200\n",
            "Episode 139, Reward: -200\n",
            "Episode 140, Reward: -200\n",
            "Episode 141, Reward: -200\n",
            "Episode 142, Reward: -200\n",
            "Episode 143, Reward: -200\n",
            "Episode 144, Reward: -200\n",
            "Episode 145, Reward: -200\n",
            "Episode 146, Reward: -200\n",
            "Episode 147, Reward: -200\n",
            "Episode 148, Reward: -200\n",
            "Episode 149, Reward: -200\n",
            "Episode 150, Reward: -200\n",
            "Episode 151, Reward: -200\n",
            "Episode 152, Reward: -200\n",
            "Episode 153, Reward: -200\n",
            "Episode 154, Reward: -200\n",
            "Episode 155, Reward: -200\n",
            "Episode 156, Reward: -200\n",
            "Episode 157, Reward: -200\n",
            "Episode 158, Reward: -200\n",
            "Episode 159, Reward: -200\n",
            "Episode 160, Reward: -200\n",
            "Episode 161, Reward: -200\n",
            "Episode 162, Reward: -200\n",
            "Episode 163, Reward: -200\n",
            "Episode 164, Reward: -200\n",
            "Episode 165, Reward: -200\n",
            "Episode 166, Reward: -200\n",
            "Episode 167, Reward: -200\n",
            "Episode 168, Reward: -200\n",
            "Episode 169, Reward: -200\n",
            "Episode 170, Reward: -200\n",
            "Episode 171, Reward: -200\n",
            "Episode 172, Reward: -200\n",
            "Episode 173, Reward: -200\n",
            "Episode 174, Reward: -200\n",
            "Episode 175, Reward: -200\n",
            "Episode 176, Reward: -200\n",
            "Episode 177, Reward: -200\n",
            "Episode 178, Reward: -200\n",
            "Episode 179, Reward: -200\n",
            "Episode 180, Reward: -200\n",
            "Episode 181, Reward: -200\n",
            "Episode 182, Reward: -200\n",
            "Episode 183, Reward: -200\n",
            "Episode 184, Reward: -200\n",
            "Episode 185, Reward: -200\n",
            "Episode 186, Reward: -200\n",
            "Episode 187, Reward: -200\n",
            "Episode 188, Reward: -200\n",
            "Episode 189, Reward: -200\n",
            "Episode 190, Reward: -200\n",
            "Episode 191, Reward: -200\n",
            "Episode 192, Reward: -200\n",
            "Episode 193, Reward: -200\n",
            "Episode 194, Reward: -200\n",
            "Episode 195, Reward: -200\n",
            "Episode 196, Reward: -200\n",
            "Episode 197, Reward: -200\n",
            "Episode 198, Reward: -200\n",
            "Episode 199, Reward: -200\n",
            "Episode 200, Reward: -200\n",
            "Episode 201, Reward: -200\n",
            "Episode 202, Reward: -200\n",
            "Episode 203, Reward: -200\n",
            "Episode 204, Reward: -200\n",
            "Episode 205, Reward: -200\n",
            "Episode 206, Reward: -200\n",
            "Episode 207, Reward: -200\n",
            "Episode 208, Reward: -200\n",
            "Episode 209, Reward: -200\n",
            "Episode 210, Reward: -200\n",
            "Episode 211, Reward: -200\n",
            "Episode 212, Reward: -200\n",
            "Episode 213, Reward: -200\n",
            "Episode 214, Reward: -200\n",
            "Episode 215, Reward: -200\n",
            "Episode 216, Reward: -200\n",
            "Episode 217, Reward: -200\n",
            "Episode 218, Reward: -200\n",
            "Episode 219, Reward: -200\n",
            "Episode 220, Reward: -200\n",
            "Episode 221, Reward: -200\n",
            "Episode 222, Reward: -200\n",
            "Episode 223, Reward: -200\n",
            "Episode 224, Reward: -200\n",
            "Episode 225, Reward: -200\n",
            "Episode 226, Reward: -200\n",
            "Episode 227, Reward: -200\n",
            "Episode 228, Reward: -200\n",
            "Episode 229, Reward: -200\n",
            "Episode 230, Reward: -200\n",
            "Episode 231, Reward: -200\n",
            "Episode 232, Reward: -200\n",
            "Episode 233, Reward: -200\n",
            "Episode 234, Reward: -200\n",
            "Episode 235, Reward: -200\n",
            "Episode 236, Reward: -200\n",
            "Episode 237, Reward: -200\n",
            "Episode 238, Reward: -200\n",
            "Episode 239, Reward: -200\n",
            "Episode 240, Reward: -200\n",
            "Episode 241, Reward: -200\n",
            "Episode 242, Reward: -200\n",
            "Episode 243, Reward: -200\n",
            "Episode 244, Reward: -200\n",
            "Episode 245, Reward: -200\n",
            "Episode 246, Reward: -200\n",
            "Episode 247, Reward: -200\n",
            "Episode 248, Reward: -200\n",
            "Episode 249, Reward: -200\n",
            "Episode 250, Reward: -200\n",
            "Episode 251, Reward: -200\n",
            "Episode 252, Reward: -200\n",
            "Episode 253, Reward: -200\n",
            "Episode 254, Reward: -200\n",
            "Episode 255, Reward: -200\n",
            "Episode 256, Reward: -200\n",
            "Episode 257, Reward: -200\n",
            "Episode 258, Reward: -200\n",
            "Episode 259, Reward: -200\n",
            "Episode 260, Reward: -200\n",
            "Episode 261, Reward: -200\n",
            "Episode 262, Reward: -200\n",
            "Episode 263, Reward: -200\n",
            "Episode 264, Reward: -200\n",
            "Episode 265, Reward: -200\n",
            "Episode 266, Reward: -200\n",
            "Episode 267, Reward: -200\n",
            "Episode 268, Reward: -200\n",
            "Episode 269, Reward: -200\n",
            "Episode 270, Reward: -200\n",
            "Episode 271, Reward: -200\n",
            "Episode 272, Reward: -200\n",
            "Episode 273, Reward: -200\n",
            "Episode 274, Reward: -200\n",
            "Episode 275, Reward: -200\n",
            "Episode 276, Reward: -200\n",
            "Episode 277, Reward: -200\n",
            "Episode 278, Reward: -200\n",
            "Episode 279, Reward: -200\n",
            "Episode 280, Reward: -200\n",
            "Episode 281, Reward: -200\n",
            "Episode 282, Reward: -200\n",
            "Episode 283, Reward: -200\n",
            "Episode 284, Reward: -200\n",
            "Episode 285, Reward: -200\n",
            "Episode 286, Reward: -200\n",
            "Episode 287, Reward: -200\n",
            "Episode 288, Reward: -200\n",
            "Episode 289, Reward: -200\n",
            "Episode 290, Reward: -200\n",
            "Episode 291, Reward: -200\n",
            "Episode 292, Reward: -200\n",
            "Episode 293, Reward: -200\n",
            "Episode 294, Reward: -200\n",
            "Episode 295, Reward: -200\n",
            "Episode 296, Reward: -200\n",
            "Episode 297, Reward: -200\n",
            "Episode 298, Reward: -200\n",
            "Episode 299, Reward: -200\n",
            "Episode 300, Reward: -200\n",
            "Episode 301, Reward: -200\n",
            "Episode 302, Reward: -200\n",
            "Episode 303, Reward: -200\n",
            "Episode 304, Reward: -200\n",
            "Episode 305, Reward: -200\n",
            "Episode 306, Reward: -200\n",
            "Episode 307, Reward: -200\n",
            "Episode 308, Reward: -200\n",
            "Episode 309, Reward: -200\n",
            "Episode 310, Reward: -200\n",
            "Episode 311, Reward: -200\n",
            "Episode 312, Reward: -200\n",
            "Episode 313, Reward: -200\n",
            "Episode 314, Reward: -200\n",
            "Episode 315, Reward: -200\n",
            "Episode 316, Reward: -200\n",
            "Episode 317, Reward: -200\n",
            "Episode 318, Reward: -200\n",
            "Episode 319, Reward: -200\n",
            "Episode 320, Reward: -200\n",
            "Episode 321, Reward: -200\n",
            "Episode 322, Reward: -200\n",
            "Episode 323, Reward: -200\n",
            "Episode 324, Reward: -200\n",
            "Episode 325, Reward: -200\n",
            "Episode 326, Reward: -200\n",
            "Episode 327, Reward: -200\n",
            "Episode 328, Reward: -200\n",
            "Episode 329, Reward: -200\n",
            "Episode 330, Reward: -200\n",
            "Episode 331, Reward: -200\n",
            "Episode 332, Reward: -200\n",
            "Episode 333, Reward: -200\n",
            "Episode 334, Reward: -200\n",
            "Episode 335, Reward: -200\n",
            "Episode 336, Reward: -200\n",
            "Episode 337, Reward: -200\n",
            "Episode 338, Reward: -200\n",
            "Episode 339, Reward: -200\n",
            "Episode 340, Reward: -200\n",
            "Episode 341, Reward: -200\n",
            "Episode 342, Reward: -200\n",
            "Episode 343, Reward: -200\n",
            "Episode 344, Reward: -200\n",
            "Episode 345, Reward: -200\n",
            "Episode 346, Reward: -200\n",
            "Episode 347, Reward: -200\n",
            "Episode 348, Reward: -200\n",
            "Episode 349, Reward: -200\n",
            "Episode 350, Reward: -200\n",
            "Episode 351, Reward: -200\n",
            "Episode 352, Reward: -200\n",
            "Episode 353, Reward: -200\n",
            "Episode 354, Reward: -200\n",
            "Episode 355, Reward: -200\n",
            "Episode 356, Reward: -200\n",
            "Episode 357, Reward: -200\n",
            "Episode 358, Reward: -200\n",
            "Episode 359, Reward: -200\n",
            "Episode 360, Reward: -200\n",
            "Episode 361, Reward: -200\n",
            "Episode 362, Reward: -200\n",
            "Episode 363, Reward: -200\n",
            "Episode 364, Reward: -200\n",
            "Episode 365, Reward: -200\n",
            "Episode 366, Reward: -200\n",
            "Episode 367, Reward: -200\n",
            "Episode 368, Reward: -200\n",
            "Episode 369, Reward: -200\n",
            "Episode 370, Reward: -200\n",
            "Episode 371, Reward: -200\n",
            "Episode 372, Reward: -200\n",
            "Episode 373, Reward: -200\n",
            "Episode 374, Reward: -200\n",
            "Episode 375, Reward: -200\n",
            "Episode 376, Reward: -200\n",
            "Episode 377, Reward: -200\n",
            "Episode 378, Reward: -200\n",
            "Episode 379, Reward: -200\n",
            "Episode 380, Reward: -200\n",
            "Episode 381, Reward: -200\n",
            "Episode 382, Reward: -200\n",
            "Episode 383, Reward: -200\n",
            "Episode 384, Reward: -200\n",
            "Episode 385, Reward: -200\n",
            "Episode 386, Reward: -200\n",
            "Episode 387, Reward: -200\n",
            "Episode 388, Reward: -200\n",
            "Episode 389, Reward: -200\n",
            "Episode 390, Reward: -200\n",
            "Episode 391, Reward: -200\n",
            "Episode 392, Reward: -200\n",
            "Episode 393, Reward: -200\n",
            "Episode 394, Reward: -200\n",
            "Episode 395, Reward: -200\n",
            "Episode 396, Reward: -200\n",
            "Episode 397, Reward: -200\n",
            "Episode 398, Reward: -200\n",
            "Episode 399, Reward: -200\n",
            "Episode 400, Reward: -200\n",
            "Episode 401, Reward: -200\n",
            "Episode 402, Reward: -200\n",
            "Episode 403, Reward: -200\n",
            "Episode 404, Reward: -200\n",
            "Episode 405, Reward: -200\n",
            "Episode 406, Reward: -200\n",
            "Episode 407, Reward: -200\n",
            "Episode 408, Reward: -200\n",
            "Episode 409, Reward: -200\n",
            "Episode 410, Reward: -200\n",
            "Episode 411, Reward: -200\n",
            "Episode 412, Reward: -200\n",
            "Episode 413, Reward: -200\n",
            "Episode 414, Reward: -200\n",
            "Episode 415, Reward: -200\n",
            "Episode 416, Reward: -200\n",
            "Episode 417, Reward: -200\n",
            "Episode 418, Reward: -200\n",
            "Episode 419, Reward: -200\n",
            "Episode 420, Reward: -200\n",
            "Episode 421, Reward: -200\n",
            "Episode 422, Reward: -200\n",
            "Episode 423, Reward: -200\n",
            "Episode 424, Reward: -200\n",
            "Episode 425, Reward: -200\n",
            "Episode 426, Reward: -200\n",
            "Episode 427, Reward: -200\n",
            "Episode 428, Reward: -200\n",
            "Episode 429, Reward: -200\n",
            "Episode 430, Reward: -200\n",
            "Episode 431, Reward: -200\n",
            "Episode 432, Reward: -200\n",
            "Episode 433, Reward: -200\n",
            "Episode 434, Reward: -200\n",
            "Episode 435, Reward: -200\n",
            "Episode 436, Reward: -200\n",
            "Episode 437, Reward: -200\n",
            "Episode 438, Reward: -200\n",
            "Episode 439, Reward: -200\n",
            "Episode 440, Reward: -200\n",
            "Episode 441, Reward: -200\n",
            "Episode 442, Reward: -200\n",
            "Episode 443, Reward: -200\n",
            "Episode 444, Reward: -200\n",
            "Episode 445, Reward: -200\n",
            "Episode 446, Reward: -200\n",
            "Episode 447, Reward: -200\n",
            "Episode 448, Reward: -200\n",
            "Episode 449, Reward: -200\n",
            "Episode 450, Reward: -200\n",
            "Episode 451, Reward: -200\n",
            "Episode 452, Reward: -200\n",
            "Episode 453, Reward: -200\n",
            "Episode 454, Reward: -200\n",
            "Episode 455, Reward: -200\n",
            "Episode 456, Reward: -200\n",
            "Episode 457, Reward: -200\n",
            "Episode 458, Reward: -200\n",
            "Episode 459, Reward: -200\n",
            "Episode 460, Reward: -200\n",
            "Episode 461, Reward: -200\n",
            "Episode 462, Reward: -200\n",
            "Episode 463, Reward: -200\n",
            "Episode 464, Reward: -200\n",
            "Episode 465, Reward: -200\n",
            "Episode 466, Reward: -200\n",
            "Episode 467, Reward: -200\n",
            "Episode 468, Reward: -200\n",
            "Episode 469, Reward: -200\n",
            "Episode 470, Reward: -200\n",
            "Episode 471, Reward: -200\n",
            "Episode 472, Reward: -200\n",
            "Episode 473, Reward: -200\n",
            "Episode 474, Reward: -200\n",
            "Episode 475, Reward: -200\n",
            "Episode 476, Reward: -200\n",
            "Episode 477, Reward: -200\n",
            "Episode 478, Reward: -200\n",
            "Episode 479, Reward: -200\n",
            "Episode 480, Reward: -200\n",
            "Episode 481, Reward: -200\n",
            "Episode 482, Reward: -200\n",
            "Episode 483, Reward: -200\n",
            "Episode 484, Reward: -200\n",
            "Episode 485, Reward: -200\n",
            "Episode 486, Reward: -200\n",
            "Episode 487, Reward: -200\n",
            "Episode 488, Reward: -200\n",
            "Episode 489, Reward: -200\n",
            "Episode 490, Reward: -200\n",
            "Episode 491, Reward: -200\n",
            "Episode 492, Reward: -200\n",
            "Episode 493, Reward: -200\n",
            "Episode 494, Reward: -200\n",
            "Episode 495, Reward: -200\n",
            "Episode 496, Reward: -200\n",
            "Episode 497, Reward: -200\n",
            "Episode 498, Reward: -200\n",
            "Episode 499, Reward: -200\n",
            "Episode 500, Reward: -200\n",
            "Episode 501, Reward: -200\n",
            "Episode 502, Reward: -200\n",
            "Episode 503, Reward: -200\n",
            "Episode 504, Reward: -200\n",
            "Episode 505, Reward: -200\n",
            "Episode 506, Reward: -200\n",
            "Episode 507, Reward: -200\n",
            "Episode 508, Reward: -200\n",
            "Episode 509, Reward: -200\n",
            "Episode 510, Reward: -200\n",
            "Episode 511, Reward: -200\n",
            "Episode 512, Reward: -200\n",
            "Episode 513, Reward: -200\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-11e88d078077>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Main\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Taxi-v3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mepisode_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_sac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-1a8226101338>\u001b[0m in \u001b[0;36mtrain_sac\u001b[0;34m(env, num_episodes, num_frames)\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstate_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                 ])\n\u001b[0;32m---> 59\u001b[0;31m                 next_states = torch.stack([\n\u001b[0m\u001b[1;32m     60\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext_state_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mnext_state_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-41-1a8226101338>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 ])\n\u001b[1;32m     59\u001b[0m                 next_states = torch.stack([\n\u001b[0;32m---> 60\u001b[0;31m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext_state_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mnext_state_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 ])\n",
            "\u001b[0;32m<ipython-input-41-1a8226101338>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 ])\n\u001b[1;32m     59\u001b[0m                 next_states = torch.stack([\n\u001b[0;32m---> 60\u001b[0;31m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext_state_seq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mnext_state_seq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 ])\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "04e319gKSYIe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}